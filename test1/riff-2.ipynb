{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu Pillow scipy \"torch>=2.1\" torchaudio \"diffusers>=0.16.1\" \"transformers>=4.33.0\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" \"gradio>=3.34.0\"\n",
    "%pip install -q -U --pre --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly \"openvino>=2025.1\" \"openvino-genai>=2025.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openvino-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__\n",
    "from transformers import GlmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import openvino_genai as ov_genai\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"cmd_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\")\n",
    "    open(\"cmd_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/riffusion-text-to-music/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"riffusion-text-to-music.ipynb\")\n",
    "\n",
    "MODEL_ID = \"riffusion/riffusion-model-v1\"\n",
    "MODEL_DIR = Path(\"riffusion_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import configure_http_backend\n",
    "from optimum.intel.openvino import OVStableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=OVStableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    EXPORT=True,\n",
    "    device=\"CPU\",\n",
    "    compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizerFast\n",
    "from openvino_tokenizers import convert_tokenizer\n",
    "from openvino.runtime import serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tok = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "ov_tok, ov_detok = convert_tokenizer(hf_tok, with_detokenizer=True)\n",
    "pipe.save_pretrained(\"riffuson_pipeline\", save_config=True)\n",
    "pipe = ov_genai.OVGenAIPipeline.from_pretrained(MODEL_ID, device=\"CPU\", compile=False)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_bytes_from_spectrogram_image(image: Image.Image) -> tuple[io.BytesIO, float]:\n",
    "    \"\"\"\n",
    "    Reconstruct a WAV audio clip from a spectrogram image. Also returns the duration in seconds.\n",
    "\n",
    "    Parameters:\n",
    "      image (Image.Image): generated spectrogram image\n",
    "    Returns:\n",
    "      wav_bytes (io.BytesIO): audio signal encoded in wav bytes\n",
    "      duration_s (float): duration in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    max_volume = 50\n",
    "    power_for_image = 0.25\n",
    "    Sxx = spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
    "\n",
    "    sample_rate = 44100  # [Hz]\n",
    "    clip_duration_ms = 5000  # [ms]\n",
    "\n",
    "    bins_per_image = 512\n",
    "    n_mels = 512\n",
    "\n",
    "    # FFT parameters\n",
    "    window_duration_ms = 100  # [ms]\n",
    "    padded_duration_ms = 400  # [ms]\n",
    "    step_size_ms = 10  # [ms]\n",
    "\n",
    "    # Derived parameters\n",
    "    num_samples = int(image.width / float(bins_per_image) * clip_duration_ms) * sample_rate\n",
    "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
    "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
    "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
    "\n",
    "    samples = waveform_from_spectrogram(\n",
    "        Sxx=Sxx,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        num_samples=num_samples,\n",
    "        sample_rate=sample_rate,\n",
    "        mel_scale=True,\n",
    "        n_mels=n_mels,\n",
    "        num_griffin_lim_iters=32,\n",
    "    )\n",
    "\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples.astype(np.int16))\n",
    "    wav_bytes.seek(0)\n",
    "\n",
    "    duration_s = float(len(samples)) / sample_rate\n",
    "\n",
    "    return wav_bytes, duration_s\n",
    "\n",
    "\n",
    "def spectrogram_from_image(image: Image.Image, max_volume: float = 50, power_for_image: float = 0.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a spectrogram magnitude array from a spectrogram image.\n",
    "\n",
    "    Parameters:\n",
    "      image (image.Image): input image\n",
    "      max_volume (float, *optional*, 50): max volume for spectrogram magnitude\n",
    "      power_for_image (float, *optional*, 0.25): power for reversing power curve\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array of floats\n",
    "    data = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Flip Y take a single channel\n",
    "    data = data[::-1, :, 0]\n",
    "\n",
    "    # Invert\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to max volume\n",
    "    data = data * max_volume / 255\n",
    "\n",
    "    # Reverse the power curve\n",
    "    data = np.power(data, 1 / power_for_image)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def waveform_from_spectrogram(\n",
    "    Sxx: np.ndarray,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    win_length: int,\n",
    "    num_samples: int,\n",
    "    sample_rate: int,\n",
    "    mel_scale: bool = True,\n",
    "    n_mels: int = 512,\n",
    "    num_griffin_lim_iters: int = 32,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct a waveform from a spectrogram.\n",
    "    This is an approximate waveform, using the Griffin-Lim algorithm\n",
    "    to approximate the phase.\n",
    "    \"\"\"\n",
    "    Sxx_torch = torch.from_numpy(Sxx).to(device)\n",
    "\n",
    "    if mel_scale:\n",
    "        mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sample_rate,\n",
    "            f_min=0,\n",
    "            f_max=10000,\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            norm=None,\n",
    "            mel_scale=\"htk\",\n",
    "        ).to(device)\n",
    "\n",
    "        Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
    "\n",
    "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        power=1.0,\n",
    "        n_iter=num_griffin_lim_iters,\n",
    "    ).to(device)\n",
    "\n",
    "    waveform = griffin_lim(Sxx_torch).cpu().numpy()\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, negative_prompt: str = \"\") -> tuple[Image.Image, str]:\n",
    "    \"\"\"\n",
    "    function for generation audio from text prompt\n",
    "\n",
    "    Parameters:\n",
    "      prompt (str): input prompt for generation.\n",
    "      negative_prompt (str): negative prompt for generation, contains undesired concepts for generation, which should be avoided. Can be empty.\n",
    "    Returns:\n",
    "      spec (Image.Image) - generated spectrogram image\n",
    "    \"\"\"\n",
    "    spec_tokens = pipe.generate(prompt, negative_prompt=negative_prompt, num_inference_steps=20)\n",
    "    spec = Image.fromarray(spec_tokens.data[0])\n",
    "    wav = wav_bytes_from_spectrogram_image(spec)\n",
    "    with open(\"output.wav\", \"wb\") as f:\n",
    "        f.write(wav[0].getbuffer())\n",
    "    return spec, \"output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dir = Path(\"riffusion_pipeline/tokenizer\")\n",
    "tok_dir.mkdir(parents=True, exist_ok=True)\n",
    "serialize(ov_tok, tok_dir / \"openvino_tokenizer.xml\")\n",
    "serialize(ov_detok, tok_dir / \"openvino_detokenizer.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram, wav_path = generate(\"Techno beat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device_str: str, current_text: str = \"\", progress: gr.Progress = gr.Progress()):\n",
    "    \"\"\"\n",
    "    Helper function for uploading model on the device.\n",
    "\n",
    "    Parameters:\n",
    "      device_str (str): Device name.\n",
    "      current_text (str): Current content of user instruction field (used only for backup purposes, temporally replacing it on the progress bar during model loading).\n",
    "      progress (gr.Progress): gradio progress tracker\n",
    "    Returns:\n",
    "      current_text\n",
    "    \"\"\"\n",
    "    if device_str != pipe._device:\n",
    "        pipe.clear_requests()\n",
    "        pipe.to(device_str)\n",
    "\n",
    "        for i in progress.tqdm(range(1), desc=f\"Model loading on {device_str}\"):\n",
    "            pipe.compile()\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = make_demo(generate_fn=generate, select_device_fn=select_device)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True, height=800)\n",
    "except Exception:\n",
    "    demo.queue().launch(debug=True, share=True, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HiFi-GAN vocoder bundle\n",
    "bundle = HIFIGAN_VOCODER_V3_LJSPEECH\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "\n",
    "def wav_bytes_from_spectrogram_image(image: Image.Image) -> tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Convert spectrogram image to waveform using HiFi-GAN for better audio fidelity.\n",
    "    \"\"\"\n",
    "    # Tweak spectrogram normalization\n",
    "    max_volume = 80\n",
    "    power_for_image = 0.5\n",
    "    Sxx = spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
    "\n",
    "    # Convert to tensor and batch\n",
    "    mel = torch.from_numpy(Sxx).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate waveform\n",
    "    with torch.no_grad():\n",
    "        waveform = vocoder(mel).squeeze(0).cpu()\n",
    "\n",
    "    # Use sample rate from bundle\n",
    "    sample_rate = bundle._sample_rate  # typically 22050\n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"\",\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: float = 7.5,\n",
    "    seed: int | None = None\n",
    ") -> tuple[Image.Image, str]:\n",
    "    \"\"\"\n",
    "    Generate spectrogram from text, invert to audio, and save WAV.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    outputs = pipe.generate(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "\n",
    "    spec = Image.fromarray(outputs.data[0])\n",
    "    waveform, sr = wav_bytes_from_spectrogram_image(spec)\n",
    "    out_path = f\"output_{seed or 'latest'}.wav\"\n",
    "    torchaudio.save(out_path, waveform.unsqueeze(0), sr)\n",
    "    return spec, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Riff-2 Text-to-Audio Demo\")\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Type description...\")\n",
    "        neg_prompt = gr.Textbox(label=\"Negative Prompt\", placeholder=\"Avoid words... (optional)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        steps = gr.Slider(10, 100, value=50, step=5, label=\"Inference Steps\")\n",
    "        scale = gr.Slider(1.0, 15.0, value=7.5, step=0.5, label=\"Guidance Scale\")\n",
    "        seed_input = gr.Number(label=\"Seed (optional)\", precision=0)\n",
    "\n",
    "    generate_btn = gr.Button(\"Generate Audio\")\n",
    "    spec_out = gr.Image(label=\"Spectrogram Output\")\n",
    "    audio_out = gr.Audio(label=\"Generated Audio\")\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=generate,\n",
    "        inputs=[prompt, neg_prompt, steps, scale, seed_input],\n",
    "        outputs=[spec_out, audio_out]\n",
    "    )\n",
    "\n",
    "    demo.launch(server_name=\"0.0.0.0\", share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
