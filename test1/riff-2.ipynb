{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu Pillow scipy \"torch>=2.1\" torchaudio \"diffusers>=0.16.1\" \"transformers>=4.33.0\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" \"gradio>=3.34.0\"\n",
    "%pip install -q -U --pre --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly \"openvino>=2025.1\" \"openvino-genai>=2025.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openvino-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__\n",
    "from transformers import GlmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import openvino_genai as ov_genai\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"cmd_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\")\n",
    "    open(\"cmd_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/riffusion-text-to-music/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"riffusion-text-to-music.ipynb\")\n",
    "\n",
    "MODEL_ID = \"riffusion/riffusion-model-v1\"\n",
    "MODEL_DIR = Path(\"riffusion_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import configure_http_backend\n",
    "from optimum.intel.openvino import OVStableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=OVStableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    EXPORT=True,\n",
    "    device=\"CPU\",\n",
    "    compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizerFast\n",
    "from openvino_tokenizers import convert_tokenizer\n",
    "from openvino.runtime import serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tok = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "ov_tok, ov_detok = convert_tokenizer(hf_tok, with_detokenizer=True)\n",
    "pipe.save_pretrained(\"riffuson_pipeline\", save_config=True)\n",
    "pipe = ov_genai.OVGenAIPipeline.from_pretrained(MODEL_ID, device=\"CPU\", compile=False)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_bytes_from_spectrogram_image(image: Image.Image) -> tuple[io.BytesIO, float]:\n",
    "    \"\"\"\n",
    "    Reconstruct a WAV audio clip from a spectrogram image. Also returns the duration in seconds.\n",
    "\n",
    "    Parameters:\n",
    "      image (Image.Image): generated spectrogram image\n",
    "    Returns:\n",
    "      wav_bytes (io.BytesIO): audio signal encoded in wav bytes\n",
    "      duration_s (float): duration in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    max_volume = 50\n",
    "    power_for_image = 0.25\n",
    "    Sxx = spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
    "\n",
    "    sample_rate = 44100  # [Hz]\n",
    "    clip_duration_ms = 5000  # [ms]\n",
    "\n",
    "    bins_per_image = 512\n",
    "    n_mels = 512\n",
    "\n",
    "    # FFT parameters\n",
    "    window_duration_ms = 100  # [ms]\n",
    "    padded_duration_ms = 400  # [ms]\n",
    "    step_size_ms = 10  # [ms]\n",
    "\n",
    "    # Derived parameters\n",
    "    num_samples = int(image.width / float(bins_per_image) * clip_duration_ms) * sample_rate\n",
    "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
    "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
    "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
    "\n",
    "    samples = waveform_from_spectrogram(\n",
    "        Sxx=Sxx,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        num_samples=num_samples,\n",
    "        sample_rate=sample_rate,\n",
    "        mel_scale=True,\n",
    "        n_mels=n_mels,\n",
    "        num_griffin_lim_iters=32,\n",
    "    )\n",
    "\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples.astype(np.int16))\n",
    "    wav_bytes.seek(0)\n",
    "\n",
    "    duration_s = float(len(samples)) / sample_rate\n",
    "\n",
    "    return wav_bytes, duration_s\n",
    "\n",
    "\n",
    "def spectrogram_from_image(image: Image.Image, max_volume: float = 50, power_for_image: float = 0.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a spectrogram magnitude array from a spectrogram image.\n",
    "\n",
    "    Parameters:\n",
    "      image (image.Image): input image\n",
    "      max_volume (float, *optional*, 50): max volume for spectrogram magnitude\n",
    "      power_for_image (float, *optional*, 0.25): power for reversing power curve\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array of floats\n",
    "    data = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Flip Y take a single channel\n",
    "    data = data[::-1, :, 0]\n",
    "\n",
    "    # Invert\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to max volume\n",
    "    data = data * max_volume / 255\n",
    "\n",
    "    # Reverse the power curve\n",
    "    data = np.power(data, 1 / power_for_image)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def waveform_from_spectrogram(\n",
    "    Sxx: np.ndarray,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    win_length: int,\n",
    "    num_samples: int,\n",
    "    sample_rate: int,\n",
    "    mel_scale: bool = True,\n",
    "    n_mels: int = 512,\n",
    "    num_griffin_lim_iters: int = 32,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct a waveform from a spectrogram.\n",
    "    This is an approximate waveform, using the Griffin-Lim algorithm\n",
    "    to approximate the phase.\n",
    "    \"\"\"\n",
    "    Sxx_torch = torch.from_numpy(Sxx).to(device)\n",
    "\n",
    "    if mel_scale:\n",
    "        mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sample_rate,\n",
    "            f_min=0,\n",
    "            f_max=10000,\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            norm=None,\n",
    "            mel_scale=\"htk\",\n",
    "        ).to(device)\n",
    "\n",
    "        Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
    "\n",
    "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        power=1.0,\n",
    "        n_iter=num_griffin_lim_iters,\n",
    "    ).to(device)\n",
    "\n",
    "    waveform = griffin_lim(Sxx_torch).cpu().numpy()\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, negative_prompt: str = \"\") -> tuple[Image.Image, str]:\n",
    "    \"\"\"\n",
    "    function for generation audio from text prompt\n",
    "\n",
    "    Parameters:\n",
    "      prompt (str): input prompt for generation.\n",
    "      negative_prompt (str): negative prompt for generation, contains undesired concepts for generation, which should be avoided. Can be empty.\n",
    "    Returns:\n",
    "      spec (Image.Image) - generated spectrogram image\n",
    "    \"\"\"\n",
    "    spec_tokens = pipe.generate(prompt, negative_prompt=negative_prompt, num_inference_steps=20)\n",
    "    spec = Image.fromarray(spec_tokens.data[0])\n",
    "    wav = wav_bytes_from_spectrogram_image(spec)\n",
    "    with open(\"output.wav\", \"wb\") as f:\n",
    "        f.write(wav[0].getbuffer())\n",
    "    return spec, \"output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dir = Path(\"riffusion_pipeline/tokenizer\")\n",
    "tok_dir.mkdir(parents=True, exist_ok=True)\n",
    "serialize(ov_tok, tok_dir / \"openvino_tokenizer.xml\")\n",
    "serialize(ov_detok, tok_dir / \"openvino_detokenizer.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram, wav_path = generate(\"Techno beat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device_str: str, current_text: str = \"\", progress: gr.Progress = gr.Progress()):\n",
    "    \"\"\"\n",
    "    Helper function for uploading model on the device.\n",
    "\n",
    "    Parameters:\n",
    "      device_str (str): Device name.\n",
    "      current_text (str): Current content of user instruction field (used only for backup purposes, temporally replacing it on the progress bar during model loading).\n",
    "      progress (gr.Progress): gradio progress tracker\n",
    "    Returns:\n",
    "      current_text\n",
    "    \"\"\"\n",
    "    if device_str != pipe._device:\n",
    "        pipe.clear_requests()\n",
    "        pipe.to(device_str)\n",
    "\n",
    "        for i in progress.tqdm(range(1), desc=f\"Model loading on {device_str}\"):\n",
    "            pipe.compile()\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = make_demo(generate_fn=generate, select_device_fn=select_device)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True, height=800)\n",
    "except Exception:\n",
    "    demo.queue().launch(debug=True, share=True, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def improved_wav_bytes_from_spectrogram_image(image: Image.Image) -> tuple[io.BytesIO, float]:\n",
    "    \"\"\"\n",
    "    Reconstruct a WAV audio clip from a spectrogram image with improved quality.\n",
    "    \n",
    "    Parameters:\n",
    "      image (Image.Image): generated spectrogram image\n",
    "    Returns:\n",
    "      wav_bytes (io.BytesIO): audio signal encoded in wav bytes\n",
    "      duration_s (float): duration in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reduced max_volume to prevent distortion\n",
    "    max_volume = 30  # Reduced from 50\n",
    "    power_for_image = 0.3  # Slightly adjusted from 0.25\n",
    "    \n",
    "    Sxx = spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
    "\n",
    "    sample_rate = 44100  # [Hz]\n",
    "    clip_duration_ms = 5000  # [ms]\n",
    "\n",
    "    bins_per_image = 512\n",
    "    n_mels = 512\n",
    "\n",
    "    # FFT parameters - optimized for better quality\n",
    "    window_duration_ms = 100  # [ms]\n",
    "    padded_duration_ms = 400  # [ms]\n",
    "    step_size_ms = 10  # [ms]\n",
    "\n",
    "    # Derived parameters\n",
    "    num_samples = int(image.width / float(bins_per_image) * clip_duration_ms) * sample_rate\n",
    "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
    "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
    "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
    "\n",
    "    # Generate waveform with increased Griffin-Lim iterations\n",
    "    samples = improved_waveform_from_spectrogram(\n",
    "        Sxx=Sxx,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        num_samples=num_samples,\n",
    "        sample_rate=sample_rate,\n",
    "        mel_scale=True,\n",
    "        n_mels=n_mels,\n",
    "        num_griffin_lim_iters=64,  # Increased from 32 for better phase reconstruction\n",
    "    )\n",
    "\n",
    "    # Apply audio post-processing for better quality\n",
    "    samples = apply_audio_enhancements(samples, sample_rate)\n",
    "\n",
    "    # Normalize audio to prevent clipping\n",
    "    samples = normalize_audio(samples)\n",
    "\n",
    "    # Convert to 16-bit integers\n",
    "    samples_int16 = (samples * 32767).astype(np.int16)\n",
    "\n",
    "    # Write to WAV bytes\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples_int16)\n",
    "    wav_bytes.seek(0)\n",
    "\n",
    "    duration_s = float(len(samples)) / sample_rate\n",
    "\n",
    "    return wav_bytes, duration_s\n",
    "\n",
    "\n",
    "def improved_waveform_from_spectrogram(\n",
    "    Sxx: np.ndarray,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    win_length: int,\n",
    "    num_samples: int,\n",
    "    sample_rate: int,\n",
    "    mel_scale: bool = True,\n",
    "    n_mels: int = 512,\n",
    "    num_griffin_lim_iters: int = 64,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct a waveform from a spectrogram with improved Griffin-Lim algorithm.\n",
    "    \"\"\"\n",
    "    # Use CPU for stability, MPS can have issues with some operations\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    Sxx_torch = torch.from_numpy(Sxx).to(device)\n",
    "    \n",
    "    # Ensure the spectrogram has the right shape (frequency_bins, time_frames)\n",
    "    if len(Sxx_torch.shape) == 2:\n",
    "        # Add batch dimension if needed\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 2D spectrogram, got shape {Sxx_torch.shape}\")\n",
    "\n",
    "    if mel_scale:\n",
    "        mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sample_rate,\n",
    "            f_min=0,\n",
    "            f_max=sample_rate // 2,  # Nyquist frequency\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            norm=None,\n",
    "            mel_scale=\"htk\",\n",
    "        ).to(device)\n",
    "\n",
    "        Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
    "\n",
    "    # Fix Griffin-Lim parameters for compatibility\n",
    "    # Ensure win_length <= n_fft\n",
    "    actual_win_length = min(win_length, n_fft)\n",
    "    \n",
    "    # Create Hann window explicitly\n",
    "    window = torch.hann_window(actual_win_length, device=device)\n",
    "    \n",
    "    # Use functional Griffin-Lim for more control\n",
    "    waveform = functional_griffin_lim(\n",
    "        specgram=Sxx_torch,\n",
    "        window=window,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=actual_win_length,\n",
    "        power=1.0,\n",
    "        n_iter=num_griffin_lim_iters,\n",
    "        momentum=0.99,\n",
    "        length=num_samples,\n",
    "        rand_init=False,  # Set to False for stability\n",
    "    )\n",
    "\n",
    "    return waveform.cpu().numpy()\n",
    "\n",
    "\n",
    "def functional_griffin_lim(\n",
    "    specgram: torch.Tensor,\n",
    "    window: torch.Tensor,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    win_length: int,\n",
    "    power: float,\n",
    "    n_iter: int,\n",
    "    momentum: float,\n",
    "    length: int,\n",
    "    rand_init: bool,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Custom Griffin-Lim implementation with better error handling.\n",
    "    \"\"\"\n",
    "    # Validate dimensions\n",
    "    assert len(specgram.shape) == 2, f\"Expected 2D spectrogram, got {specgram.shape}\"\n",
    "    \n",
    "    # Initialize random phases or zeros\n",
    "    if rand_init:\n",
    "        angles = torch.rand_like(specgram) * 2 * np.pi - np.pi\n",
    "    else:\n",
    "        angles = torch.zeros_like(specgram)\n",
    "    \n",
    "    # Convert to complex spectrogram\n",
    "    complex_specgram = specgram * torch.exp(1j * angles)\n",
    "    \n",
    "    # Momentum term\n",
    "    tprev = torch.tensor(0.0, dtype=specgram.dtype, device=specgram.device)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        try:\n",
    "            # Inverse STFT\n",
    "            inverse = torch.istft(\n",
    "                complex_specgram,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "                window=window,\n",
    "                length=length,\n",
    "                return_complex=False\n",
    "            )\n",
    "            \n",
    "            # Forward STFT\n",
    "            rebuilt = torch.stft(\n",
    "                inverse,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "                window=window,\n",
    "                return_complex=True,\n",
    "                pad_mode='reflect'\n",
    "            )\n",
    "            \n",
    "            # Update angles with momentum\n",
    "            angles = torch.angle(rebuilt)\n",
    "            if momentum > 0:\n",
    "                angles = momentum * angles + (1 - momentum) * torch.angle(complex_specgram)\n",
    "            \n",
    "            # Update complex spectrogram\n",
    "            complex_specgram = specgram * torch.exp(1j * angles)\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Griffin-Lim iteration failed: {e}\")\n",
    "            # Fall back to previous iteration or basic reconstruction\n",
    "            break\n",
    "    \n",
    "    # Final reconstruction\n",
    "    try:\n",
    "        result = torch.istft(\n",
    "            complex_specgram,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            window=window,\n",
    "            length=length,\n",
    "            return_complex=False\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Final ISTFT failed: {e}, using simple inverse\")\n",
    "        # Fallback: simple magnitude-only reconstruction\n",
    "        result = torch.istft(\n",
    "            specgram.unsqueeze(-1).repeat(1, 1, 2).view_as(torch.complex64),\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            window=window,\n",
    "            length=length,\n",
    "            return_complex=False\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_audio_enhancements(samples: np.ndarray, sample_rate: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply various audio enhancements to improve quality.\n",
    "    \"\"\"\n",
    "    # 1. High-pass filter to remove low-frequency noise and rumble\n",
    "    try:\n",
    "        b, a = signal.butter(3, 80, 'highpass', fs=sample_rate)\n",
    "        samples = signal.filtfilt(b, a, samples)\n",
    "    except Exception as e:\n",
    "        print(f\"High-pass filter failed: {e}\")\n",
    "    \n",
    "    # 2. Light low-pass filter to remove high-frequency artifacts\n",
    "    try:\n",
    "        b, a = signal.butter(5, sample_rate // 2 - 1000, 'lowpass', fs=sample_rate)\n",
    "        samples = signal.filtfilt(b, a, samples)\n",
    "    except Exception as e:\n",
    "        print(f\"Low-pass filter failed: {e}\")\n",
    "    \n",
    "    # 3. De-clicking/de-popping (remove sudden spikes)\n",
    "    samples = remove_clicks(samples, sample_rate)\n",
    "    \n",
    "    # 4. Light compression to even out dynamics\n",
    "    samples = apply_soft_compression(samples)\n",
    "    \n",
    "    # 5. Fade in/out to prevent clicks at start/end\n",
    "    samples = apply_fade_in_out(samples, sample_rate)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def remove_clicks(samples: np.ndarray, sample_rate: int, threshold: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remove sudden spikes/clicks in the audio.\n",
    "    \"\"\"\n",
    "    # Calculate derivative to find sudden changes\n",
    "    diff = np.diff(samples)\n",
    "    \n",
    "    # Find spikes above threshold\n",
    "    spike_indices = np.where(np.abs(diff) > threshold)[0]\n",
    "    \n",
    "    # Smooth out spikes\n",
    "    for idx in spike_indices:\n",
    "        if 0 < idx < len(samples) - 1:\n",
    "            # Replace spike with interpolated value\n",
    "            samples[idx] = (samples[idx-1] + samples[idx+1]) / 2\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def apply_soft_compression(samples: np.ndarray, ratio: float = 3.0, threshold: float = 0.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply soft compression to even out dynamics.\n",
    "    \"\"\"\n",
    "    # Simple soft compression\n",
    "    compressed = np.where(\n",
    "        np.abs(samples) > threshold,\n",
    "        np.sign(samples) * (threshold + (np.abs(samples) - threshold) / ratio),\n",
    "        samples\n",
    "    )\n",
    "    return compressed\n",
    "\n",
    "\n",
    "def apply_fade_in_out(samples: np.ndarray, sample_rate: int, fade_duration_ms: int = 50) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply fade in/out to prevent clicks at start/end.\n",
    "    \"\"\"\n",
    "    fade_samples = int(fade_duration_ms * sample_rate / 1000)\n",
    "    fade_samples = min(fade_samples, len(samples) // 4)  # Don't fade more than 1/4 of the audio\n",
    "    \n",
    "    if fade_samples > 0:\n",
    "        # Fade in\n",
    "        fade_in = np.linspace(0, 1, fade_samples)\n",
    "        samples[:fade_samples] *= fade_in\n",
    "        \n",
    "        # Fade out\n",
    "        fade_out = np.linspace(1, 0, fade_samples)\n",
    "        samples[-fade_samples:] *= fade_out\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def normalize_audio(samples: np.ndarray, target_level: float = 0.8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize audio to target level while preserving dynamics.\n",
    "    \"\"\"\n",
    "    # Find peak\n",
    "    peak = np.max(np.abs(samples))\n",
    "    \n",
    "    if peak > 0:\n",
    "        # Normalize to target level\n",
    "        samples = samples * (target_level / peak)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def spectrogram_from_image(image: Image.Image, max_volume: float = 30, power_for_image: float = 0.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a spectrogram magnitude array from a spectrogram image.\n",
    "    Enhanced version with better processing.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    data = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Handle different image formats\n",
    "    if len(data.shape) == 3:\n",
    "        # Take the first channel if RGB/RGBA\n",
    "        data = data[:, :, 0]\n",
    "    \n",
    "    # Flip Y axis (spectrogram convention)\n",
    "    data = data[::-1, :]\n",
    "\n",
    "    # Invert colors (assuming dark = low energy, bright = high energy)\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to max volume\n",
    "    data = data * max_volume / 255\n",
    "\n",
    "    # Reverse the power curve with improved handling\n",
    "    data = np.power(np.maximum(data, 1e-10), 1 / power_for_image)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
