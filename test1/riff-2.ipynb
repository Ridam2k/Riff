{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu Pillow scipy \"torch>=2.1\" torchaudio \"diffusers>=0.16.1\" \"transformers>=4.33.0\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" \"gradio>=3.34.0\"\n",
    "%pip install -q -U --pre --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly \"openvino>=2025.1\" \"openvino-genai>=2025.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openvino-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__\n",
    "from transformers import GlmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import openvino_genai as ov_genai\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"cmd_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\")\n",
    "    open(\"cmd_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/riffusion-text-to-music/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"riffusion-text-to-music.ipynb\")\n",
    "\n",
    "MODEL_ID = \"riffusion/riffusion-model-v1\"\n",
    "MODEL_DIR = Path(\"riffusion_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import configure_http_backend\n",
    "from optimum.intel.openvino import OVStableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=OVStableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    EXPORT=True,\n",
    "    device=\"CPU\",\n",
    "    compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizerFast\n",
    "from openvino_tokenizers import convert_tokenizer\n",
    "from openvino.runtime import serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tok = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "ov_tok, ov_detok = convert_tokenizer(hf_tok, with_detokenizer=True)\n",
    "pipe.save_pretrained(\"riffuson_pipeline\", save_config=True)\n",
    "pipe = ov_genai.OVGenAIPipeline.from_pretrained(MODEL_ID, device=\"CPU\", compile=False)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_bytes_from_spectrogram_image(image: Image.Image) -> tuple[io.BytesIO, float]:\n",
    "    \"\"\"\n",
    "    Reconstruct a WAV audio clip from a spectrogram image. Also returns the duration in seconds.\n",
    "\n",
    "    Parameters:\n",
    "      image (Image.Image): generated spectrogram image\n",
    "    Returns:\n",
    "      wav_bytes (io.BytesIO): audio signal encoded in wav bytes\n",
    "      duration_s (float): duration in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    max_volume = 50\n",
    "    power_for_image = 0.25\n",
    "    Sxx = spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
    "\n",
    "    sample_rate = 44100  # [Hz]\n",
    "    clip_duration_ms = 5000  # [ms]\n",
    "\n",
    "    bins_per_image = 512\n",
    "    n_mels = 512\n",
    "\n",
    "    # FFT parameters\n",
    "    window_duration_ms = 100  # [ms]\n",
    "    padded_duration_ms = 400  # [ms]\n",
    "    step_size_ms = 10  # [ms]\n",
    "\n",
    "    # Derived parameters\n",
    "    num_samples = int(image.width / float(bins_per_image) * clip_duration_ms) * sample_rate\n",
    "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
    "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
    "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
    "\n",
    "    samples = waveform_from_spectrogram(\n",
    "        Sxx=Sxx,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        num_samples=num_samples,\n",
    "        sample_rate=sample_rate,\n",
    "        mel_scale=True,\n",
    "        n_mels=n_mels,\n",
    "        num_griffin_lim_iters=32,\n",
    "    )\n",
    "\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples.astype(np.int16))\n",
    "    wav_bytes.seek(0)\n",
    "\n",
    "    duration_s = float(len(samples)) / sample_rate\n",
    "\n",
    "    return wav_bytes, duration_s\n",
    "\n",
    "\n",
    "def spectrogram_from_image(image: Image.Image, max_volume: float = 50, power_for_image: float = 0.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a spectrogram magnitude array from a spectrogram image.\n",
    "\n",
    "    Parameters:\n",
    "      image (image.Image): input image\n",
    "      max_volume (float, *optional*, 50): max volume for spectrogram magnitude\n",
    "      power_for_image (float, *optional*, 0.25): power for reversing power curve\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array of floats\n",
    "    data = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Flip Y take a single channel\n",
    "    data = data[::-1, :, 0]\n",
    "\n",
    "    # Invert\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to max volume\n",
    "    data = data * max_volume / 255\n",
    "\n",
    "    # Reverse the power curve\n",
    "    data = np.power(data, 1 / power_for_image)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def waveform_from_spectrogram(\n",
    "    Sxx: np.ndarray,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    win_length: int,\n",
    "    num_samples: int,\n",
    "    sample_rate: int,\n",
    "    mel_scale: bool = True,\n",
    "    n_mels: int = 512,\n",
    "    num_griffin_lim_iters: int = 32,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct a waveform from a spectrogram.\n",
    "    This is an approximate waveform, using the Griffin-Lim algorithm\n",
    "    to approximate the phase.\n",
    "    \"\"\"\n",
    "    Sxx_torch = torch.from_numpy(Sxx).to(device)\n",
    "\n",
    "    if mel_scale:\n",
    "        mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sample_rate,\n",
    "            f_min=0,\n",
    "            f_max=10000,\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            norm=None,\n",
    "            mel_scale=\"htk\",\n",
    "        ).to(device)\n",
    "\n",
    "        Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
    "\n",
    "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        power=1.0,\n",
    "        n_iter=num_griffin_lim_iters,\n",
    "    ).to(device)\n",
    "\n",
    "    waveform = griffin_lim(Sxx_torch).cpu().numpy()\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, negative_prompt: str = \"\") -> tuple[Image.Image, str]:\n",
    "    \"\"\"\n",
    "    function for generation audio from text prompt\n",
    "\n",
    "    Parameters:\n",
    "      prompt (str): input prompt for generation.\n",
    "      negative_prompt (str): negative prompt for generation, contains undesired concepts for generation, which should be avoided. Can be empty.\n",
    "    Returns:\n",
    "      spec (Image.Image) - generated spectrogram image\n",
    "    \"\"\"\n",
    "    spec_tokens = pipe.generate(prompt, negative_prompt=negative_prompt, num_inference_steps=20)\n",
    "    spec = Image.fromarray(spec_tokens.data[0])\n",
    "    wav = wav_bytes_from_spectrogram_image(spec)\n",
    "    with open(\"output.wav\", \"wb\") as f:\n",
    "        f.write(wav[0].getbuffer())\n",
    "    return spec, \"output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dir = Path(\"riffusion_pipeline/tokenizer\")\n",
    "tok_dir.mkdir(parents=True, exist_ok=True)\n",
    "serialize(ov_tok, tok_dir / \"openvino_tokenizer.xml\")\n",
    "serialize(ov_detok, tok_dir / \"openvino_detokenizer.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram, wav_path = generate(\"Techno beat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device_str: str, current_text: str = \"\", progress: gr.Progress = gr.Progress()):\n",
    "    \"\"\"\n",
    "    Helper function for uploading model on the device.\n",
    "\n",
    "    Parameters:\n",
    "      device_str (str): Device name.\n",
    "      current_text (str): Current content of user instruction field (used only for backup purposes, temporally replacing it on the progress bar during model loading).\n",
    "      progress (gr.Progress): gradio progress tracker\n",
    "    Returns:\n",
    "      current_text\n",
    "    \"\"\"\n",
    "    if device_str != pipe._device:\n",
    "        pipe.clear_requests()\n",
    "        pipe.to(device_str)\n",
    "\n",
    "        for i in progress.tqdm(range(1), desc=f\"Model loading on {device_str}\"):\n",
    "            pipe.compile()\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = make_demo(generate_fn=generate, select_device_fn=select_device)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True, height=800)\n",
    "except Exception:\n",
    "    demo.queue().launch(debug=True, share=True, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def improved_wav_bytes_from_spectrogram_image(image: Image.Image) -> tuple[io.BytesIO, float]:\n",
    "    \"\"\"\n",
    "    Reconstruct a WAV audio clip from a spectrogram image with improved quality.\n",
    "    This version prioritizes stability and compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "      image (Image.Image): generated spectrogram image\n",
    "    Returns:\n",
    "      wav_bytes (io.BytesIO): audio signal encoded in wav bytes\n",
    "      duration_s (float): duration in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Improved but conservative parameters\n",
    "    max_volume = 25  # Reduced to prevent distortion\n",
    "    power_for_image = 0.3  # Slightly adjusted for better dynamics\n",
    "    \n",
    "    Sxx = improved_spectrogram_from_image(image, max_volume=max_volume, power_for_image=power_for_image)\n",
    "\n",
    "    sample_rate = 44100  # [Hz]\n",
    "    clip_duration_ms = 5000  # [ms]\n",
    "\n",
    "    bins_per_image = 512\n",
    "    n_mels = 512\n",
    "\n",
    "    # FFT parameters\n",
    "    window_duration_ms = 100  # [ms]\n",
    "    padded_duration_ms = 400  # [ms]\n",
    "    step_size_ms = 10  # [ms]\n",
    "\n",
    "    # Derived parameters\n",
    "    num_samples = int(image.width / float(bins_per_image) * clip_duration_ms) * sample_rate\n",
    "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
    "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
    "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
    "\n",
    "    # Use improved but stable waveform generation\n",
    "    samples = improved_waveform_from_spectrogram(\n",
    "        Sxx=Sxx,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        num_samples=num_samples,\n",
    "        sample_rate=sample_rate,\n",
    "        mel_scale=True,\n",
    "        n_mels=n_mels,\n",
    "        num_griffin_lim_iters=40,  # Moderate increase for better quality\n",
    "    )\n",
    "\n",
    "    # Apply safe audio enhancements\n",
    "    samples = apply_safe_audio_enhancements(samples, sample_rate)\n",
    "\n",
    "    # Normalize and convert to WAV\n",
    "    wav_bytes = samples_to_wav_bytes(samples, sample_rate)\n",
    "    duration_s = float(len(samples)) / sample_rate\n",
    "\n",
    "    return wav_bytes, duration_s\n",
    "\n",
    "\n",
    "def improved_waveform_from_spectrogram(\n",
    "    Sxx: np.ndarray,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    win_length: int,\n",
    "    num_samples: int,\n",
    "    sample_rate: int,\n",
    "    mel_scale: bool = True,\n",
    "    n_mels: int = 512,\n",
    "    num_griffin_lim_iters: int = 40,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Improved but stable waveform reconstruction from spectrogram.\n",
    "    \"\"\"\n",
    "    # Force CPU for maximum compatibility\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    Sxx_torch = torch.from_numpy(Sxx).to(device).float()\n",
    "\n",
    "    if mel_scale:\n",
    "        mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sample_rate,\n",
    "            f_min=0,\n",
    "            f_max=10000,  # Conservative upper frequency\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            norm=None,\n",
    "            mel_scale=\"htk\",\n",
    "        ).to(device)\n",
    "\n",
    "        Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
    "\n",
    "    # Use Griffin-Lim with safe parameters\n",
    "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        power=1.0,\n",
    "        n_iter=num_griffin_lim_iters,\n",
    "        momentum=0.9,  # Add some momentum for better convergence\n",
    "        length=num_samples,\n",
    "        rand_init=False,  # Disable random init for stability\n",
    "    ).to(device)\n",
    "\n",
    "    try:\n",
    "        waveform = griffin_lim(Sxx_torch).cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Griffin-Lim failed: {e}, using basic reconstruction\")\n",
    "        # Fallback to simple ISTFT without Griffin-Lim\n",
    "        waveform = basic_istft_reconstruction(Sxx_torch, n_fft, hop_length, win_length, num_samples)\n",
    "\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def basic_istft_reconstruction(Sxx_torch, n_fft, hop_length, win_length, num_samples):\n",
    "    \"\"\"\n",
    "    Basic ISTFT reconstruction as fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create zero phase\n",
    "        zero_phase = torch.zeros_like(Sxx_torch)\n",
    "        complex_spec = torch.complex(Sxx_torch, zero_phase)\n",
    "        \n",
    "        # Create window\n",
    "        window = torch.hann_window(win_length, device=Sxx_torch.device)\n",
    "        \n",
    "        # ISTFT\n",
    "        waveform = torch.istft(\n",
    "            complex_spec,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            window=window,\n",
    "            length=num_samples,\n",
    "            return_complex=False\n",
    "        )\n",
    "        return waveform.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Basic ISTFT also failed: {e}, returning silence\")\n",
    "        return np.zeros(num_samples, dtype=np.float32)\n",
    "\n",
    "\n",
    "def apply_safe_audio_enhancements(samples: np.ndarray, sample_rate: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply safe audio enhancements that won't cause errors.\n",
    "    \"\"\"\n",
    "    # 1. Simple clipping prevention\n",
    "    samples = np.clip(samples, -1.0, 1.0)\n",
    "    \n",
    "    # 2. Basic high-pass filter to remove rumble\n",
    "    try:\n",
    "        # Very conservative filter\n",
    "        nyquist = sample_rate / 2\n",
    "        low_cutoff = 80 / nyquist\n",
    "        b, a = signal.butter(2, low_cutoff, btype='high')\n",
    "        samples = signal.filtfilt(b, a, samples)\n",
    "    except Exception as e:\n",
    "        print(f\"High-pass filter failed: {e}, skipping\")\n",
    "    \n",
    "    # 3. Remove DC offset\n",
    "    samples = samples - np.mean(samples)\n",
    "    \n",
    "    # 4. Simple normalization with headroom\n",
    "    peak = np.max(np.abs(samples))\n",
    "    if peak > 0:\n",
    "        samples = samples * (0.8 / peak)  # Leave 20% headroom\n",
    "    \n",
    "    # 5. Gentle fade in/out to prevent clicks\n",
    "    fade_length = min(int(0.01 * sample_rate), len(samples) // 10)  # 10ms or 10% of length\n",
    "    if fade_length > 0:\n",
    "        # Fade in\n",
    "        fade_in = np.linspace(0, 1, fade_length)\n",
    "        samples[:fade_length] *= fade_in\n",
    "        \n",
    "        # Fade out\n",
    "        fade_out = np.linspace(1, 0, fade_length)\n",
    "        samples[-fade_length:] *= fade_out\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def samples_to_wav_bytes(samples: np.ndarray, sample_rate: int) -> io.BytesIO:\n",
    "    \"\"\"\n",
    "    Convert audio samples to WAV bytes with proper formatting.\n",
    "    \"\"\"\n",
    "    # Ensure samples are in valid range\n",
    "    samples = np.clip(samples, -1.0, 1.0)\n",
    "    \n",
    "    # Convert to 16-bit integers\n",
    "    samples_int16 = (samples * 32767).astype(np.int16)\n",
    "    \n",
    "    # Create WAV bytes\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples_int16)\n",
    "    wav_bytes.seek(0)\n",
    "    \n",
    "    return wav_bytes\n",
    "\n",
    "\n",
    "def improved_spectrogram_from_image(image: Image.Image, max_volume: float = 25, power_for_image: float = 0.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Improved spectrogram extraction from image with better preprocessing.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    data = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Handle different image formats\n",
    "    if len(data.shape) == 3:\n",
    "        # Convert RGB to grayscale if needed\n",
    "        if data.shape[2] >= 3:\n",
    "            # Use luminance formula for better conversion\n",
    "            data = 0.299 * data[:, :, 0] + 0.587 * data[:, :, 1] + 0.114 * data[:, :, 2]\n",
    "        else:\n",
    "            data = data[:, :, 0]\n",
    "    \n",
    "    # Flip Y axis (spectrogram convention)\n",
    "    data = data[::-1, :]\n",
    "\n",
    "    # Invert colors (dark = low energy, bright = high energy)\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to max volume\n",
    "    data = data * max_volume / 255\n",
    "\n",
    "    # Apply power curve with numerical stability\n",
    "    data = np.maximum(data, 1e-10)  # Prevent zeros\n",
    "    data = np.power(data, 1 / power_for_image)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
